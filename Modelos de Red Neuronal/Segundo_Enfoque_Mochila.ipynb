{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FgsP4plB4wIi",
        "xfErMhZ4ZRWJ",
        "eUv0RgiQgz3R",
        "PS13MmCRAP7m",
        "owAsBWNdMZSE",
        "KFhlnoxYFMBX",
        "ZCnR5HSCHEQY",
        "UuRW5cm4VT0V",
        "a9Dx55KmVvN0",
        "SZQ1FjfEVHsZ",
        "xABdQ50oXzEy",
        "l42Mv7YYXzE5",
        "JvWXt7BMXzE6",
        "xSpQfBR6XzE6",
        "IY33mMlMy6fV",
        "SeCmy7asU2gM",
        "lX0ZrM6wUcMD",
        "OnSbDWVhOCoN",
        "IhSOSIWFOCoV",
        "1c4shzWA-5oe",
        "Otl_PApe-5oe",
        "EN4VPEF2-5of",
        "oKIXmrA-4wIp",
        "gGPuDixn4wIp",
        "YSnPSSupvvlE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inicialización\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JlFwrBIXu232"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "random_state = 33\n",
        "np.random.seed(random_state)        # Semilla para NumPy\n",
        "tf.random.set_seed(random_state)    # Semilla para TensorFlow"
      ],
      "metadata": {
        "id": "HN4sfCSZwW4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para acceder a los ficheros de Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# La carpeta datos (que debe contiene los archivos de datos) debe estar en vuestro Drive, dentro de la carpeta 'Colab Notebooks'"
      ],
      "metadata": {
        "id": "63VrcRrnwW4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####métricas con el valor fitness"
      ],
      "metadata": {
        "id": "eqWGTU4k4Vfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valor_fitness(instancia, caso):\n",
        "  instancia_local = instancia.copy()\n",
        "  valores = caso[2::2]\n",
        "  pesos = caso[1::2]\n",
        "  pesoAct = np.sum(pesos*instancia_local)\n",
        "  i=0\n",
        "  while(pesoAct > caso[0]): # mientras el peso sea mayor\n",
        "    if(instancia_local[i]):\n",
        "      pesoAct -= pesos[i]\n",
        "      instancia_local[i] = 0\n",
        "    i += 1\n",
        "  return np.sum(valores*instancia_local)\n"
      ],
      "metadata": {
        "id": "qZ7pJlrso7I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrica_valor_fitness(y_pred, x):\n",
        "  sum_valores = 0\n",
        "  num_tot = len(x)\n",
        "  num_items = int(len(x[0])/2+1)\n",
        "  print(num_items)\n",
        "  umbral = 0.5\n",
        "  predictions_binary = (y_pred >= umbral).astype(int)\n",
        "  for i in range(num_tot):\n",
        "    sum_valores += valor_fitness(predictions_binary[i][ num_items:], x[i])\n",
        "  return sum_valores/num_tot"
      ],
      "metadata": {
        "id": "K1-KdxI_m5pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrica_valor_fitness(predictions_binary, x):\n",
        "  sum_valores = 0\n",
        "  num_tot = len(x)\n",
        "  num_items = int(len(x[0])/2+1)\n",
        "  for i in range(num_tot):\n",
        "    sum_valores += valor_fitness(predictions_binary[i][ num_items:], x[i])\n",
        "  return sum_valores/num_tot"
      ],
      "metadata": {
        "id": "14wMBjXVYKcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Métrica fitness comparando con la solucion optima"
      ],
      "metadata": {
        "id": "Vj9el2eriPZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def metrica_fitness_mejor_solucion(predictions, input, exact_sols):\n",
        "  sum_pred = 0\n",
        "  sum_exact = 0\n",
        "  preds = predictions[:,int(predictions.shape[1] / 2)+1:].copy()\n",
        "  for caso in range(len(input)):\n",
        "    valores = input[caso][2::2]\n",
        "    pesos = input[caso][1::2]\n",
        "    pesoAct = np.sum(pesos*preds[caso])\n",
        "    i=0\n",
        "    while(pesoAct > input[caso][0]): # mientras el peso sea mayor\n",
        "      if(preds[caso][i]):\n",
        "        pesoAct -= pesos[i]\n",
        "        preds[caso][i] = 0\n",
        "      i += 1\n",
        "    sum_pred += np.sum(valores*preds[caso])\n",
        "    sum_exact += exact_sols[caso]\n",
        "    # if caso %1000 == 0: print(caso)\n",
        "  return sum_pred/sum_exact"
      ],
      "metadata": {
        "id": "A0RSRrG5opER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experimento Mochila $N=10$ y $10^5$ casos, con solucion optima"
      ],
      "metadata": {
        "id": "Od52-a1iRtro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido al alto coste computacional de entrenar las redes neuronales, ejecutar etse experimento lleva una cantidad considerable de horas."
      ],
      "metadata": {
        "id": "e6lOfpL4N3Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El problema utiliza $N$ objetos exactamente y los datos se cargan desde un fichero .csv como un vector de X casos y cada posición es un vector de $4N+2$ variables que representan:   \n",
        "\n",
        "\n",
        "0   Número de Objetos.  \n",
        "1   Capacidad de la mochila.   \n",
        "2 - 2N+1 Peso y Valor de cada objeto en ese orden.  \n",
        "\n",
        "Voy a utilizar el propio caso como etiqueta, aunque no utilice la etiqueta para nada más que evaluar la solución generada.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_PTSfZ9lRtrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "He generado un conjunto de posibles casos para el problema y con sus soluciones exactas calculadas para cada caso."
      ],
      "metadata": {
        "id": "7a1SjAjRwW4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cargar y separar datos"
      ],
      "metadata": {
        "id": "FgsP4plB4wIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'drive/MyDrive/Colab Notebooks/datos/TFG/data1_10_10_5Comas.csv'\n",
        "\n",
        "datos = np.genfromtxt(file_path, delimiter=\",\") #Funcion cargar datos desde txt, y en este caso desde csv\n",
        "\n",
        "numItems = datos[0][0]\n",
        "x = datos[:, 1:int(numItems*2+2)]  # datos de entrada\n",
        "y = datos[:, 1:int(numItems*2+2)]   # datos de etiqueta\n",
        "x = np.array(x, np.float64)\n",
        "y = np.array(y, np.float64)\n",
        "exact_sols = datos[:,-1]\n",
        "print(\"Número de objetos (N) =\", numItems)\n",
        "# Mostrar dimension del conjunto de muestras total\n",
        "print(\"Forma de vector X de muestras:\", x.shape)\n",
        "print(\"Forma de vector Y de etiquetas:\", y.shape)\n",
        "print(\"Forma de vector exact_sols de etiquetas:\", exact_sols.shape)\n",
        "\n",
        "print(\"Ejemplos:\")\n",
        "print(\"Datos = \",datos[0])\n",
        "print(\"exact_sols = \",exact_sols[0])\n",
        "print(\"X = \",x[0])\n",
        "print(\"Y = \",y[0])"
      ],
      "metadata": {
        "id": "eVjwj6o9Rtrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separo el conjunto de casos en test y train para evaluar los modelos utilizados"
      ],
      "metadata": {
        "id": "oMR2YV3ORtrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate data in test and train\n",
        "\n",
        "np.random.seed(0)\n",
        "trainPortion = 0.8 #porcentaje de train, el porcentaje de test será la resta de 1 menos el porcentaje de train\n",
        "\n",
        "#-------------Obtener índices\n",
        "\n",
        "indexesData = np.arange(len(y)) #Indices del conjunto de muestras\n",
        "#-------------Desordenar indices y separar en rain y test\n",
        "\n",
        "np.random.shuffle(indexesData) #Desordenar indices de las muestras\n",
        "numberTrain = round(len(indexesData)*trainPortion) #numero de muestras para train\n",
        "trainIndexes = indexesData[:numberTrain]\n",
        "testIndexes = indexesData[numberTrain:]\n",
        "\n",
        "#-------------Datos desorden:ados para train y test\n",
        "\n",
        "trainX = x[trainIndexes]\n",
        "testX = x[testIndexes]\n",
        "trainY = y[trainIndexes]\n",
        "testY = y[testIndexes]\n",
        "\n",
        "#-------------Copia de los datos para mantener datos originales después del preprocesado\n",
        "\n",
        "original_trainX = trainX.copy()\n",
        "original_testX = testX.copy()\n",
        "original_trainY = trainY.copy()\n",
        "original_testY = testY.copy()\n",
        "\n",
        "#-------------Mostrar resultados\n",
        "\n",
        "print('Muestras totales:  {}'.format(len(trainY)+len(testX)))\n",
        "print('Muestras train:  {}'.format(len(trainY)))\n",
        "print('Muestras test:  {}'.format(len(testX)))\n"
      ],
      "metadata": {
        "id": "rypuRZ_gRtrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento de los datos"
      ],
      "metadata": {
        "id": "xfErMhZ4ZRWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como los datos han sido generados artificialmente y no se han obtenido de fuentes externas, no es necesario realizar la búsqueda de datos faltantes o erroneos ni desbalanceo en las proporciones ya que los datos han sido generados teniendo en cuenta estos factores. La codificación de los datos también se ha tenido en cuenta en al generación de los datos."
      ],
      "metadata": {
        "id": "OoAxwvIFZozf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comprobación de Duplicados en los datos"
      ],
      "metadata": {
        "id": "I2ROBk7nUxBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobando duplicados en datos\n",
        "unique_rows= np.unique(datos, axis=0)\n",
        "num_duplicates = datos.shape[0] - unique_rows.shape[0]\n",
        "\n",
        "print(\"Número de filas duplicadas en los datos:\", num_duplicates)"
      ],
      "metadata": {
        "id": "_fNxxuYaUjwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Escalar Datos"
      ],
      "metadata": {
        "id": "6-x3OHfkJut3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el objeto MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Ajustar el escalador con los datos de entrenamiento y transformar tanto train como test\n",
        "trainX_scaled = scaler.fit_transform(trainX)\n",
        "testX_scaled = scaler.transform(testX)\n",
        "trainX_scaled = np.array(trainX_scaled)\n",
        "testX_scaled = np.array(testX_scaled)\n",
        "\n",
        "print(\"train X original = \",trainX[0])\n",
        "print(\"train X scaled = \",trainX_scaled[0])\n",
        "print(\"test X = \",testX_scaled[0])"
      ],
      "metadata": {
        "id": "c5BVGzNIJut3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de Pérdida Personalizada"
      ],
      "metadata": {
        "id": "eUv0RgiQgz3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Versiones iniciales de la función de pérdida"
      ],
      "metadata": {
        "id": "eJIJURHZMWS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fitness_loss_custom(target, output):\n",
        "    target_0 = tf.gather(target, indices=0, axis=1)\n",
        "    valores = target[:, 2::2]\n",
        "    pesos = target[:, 1::2]\n",
        "    pred = output[:, int(output.shape[1] / 2)+1:]\n",
        "\n",
        "    # Inicializar el peso acumulado y la máscara\n",
        "    peso_act = tf.reduce_sum(pesos * pred, axis=1)\n",
        "    mask = tf.ones_like(valores)\n",
        "\n",
        "    # Condición para el bucle while: peso_act > target_0\n",
        "    def cond(i, peso_act, mask):\n",
        "        return tf.reduce_any(peso_act > target_0)\n",
        "\n",
        "    # Cuerpo del bucle while\n",
        "    def body(i, peso_act, mask):\n",
        "        # Actualizar peso_act y mask\n",
        "        update_mask = tf.where(pred[:, i] > 0, tf.zeros_like(mask[:, i]), mask[:, i])\n",
        "        update_peso_act = tf.where(pred[:, i] > 0, peso_act - pesos[:, i], peso_act)\n",
        "\n",
        "        return [i + 1, update_peso_act, tf.concat([mask[:, :i], tf.expand_dims(update_mask, -1), mask[:, i+1:]], axis=1)]\n",
        "\n",
        "    # Aplicar el bucle while\n",
        "    i = tf.constant(0)\n",
        "    [_, peso_act, mask] = tf.while_loop(cond, body, loop_vars=[i, peso_act, mask], shape_invariants=[i.get_shape(), peso_act.get_shape(), tf.TensorShape([None, None])])\n",
        "\n",
        "    # Calcular la pérdida final\n",
        "    final_loss = 1.0 / tf.reduce_sum(valores * mask * pred, axis=1)\n",
        "\n",
        "    return final_loss\n"
      ],
      "metadata": {
        "id": "jzs5OLkE2WXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def fitness_loss_custom2(target, output):\n",
        "    # Función que se aplicará a cada par (target_i, output_i)\n",
        "    def process_instance(args):\n",
        "        #print(args)\n",
        "        target_i, output_i = args\n",
        "        target_0_i = target_i[0]\n",
        "        valores_i = target_i[2::2]\n",
        "        pesos_i = target_i[1::2]\n",
        "        pred_i = output_i[int(output_i.shape[0] / 2)+1:]\n",
        "\n",
        "        mask_i = tf.ones_like(valores_i)\n",
        "        # Funciones para condiciones y cuerpo del bucle while\n",
        "        def cond(peso_act, i):\n",
        "            return tf.reduce_any(peso_act > target_0_i)\n",
        "\n",
        "        def body(peso_act, i):\n",
        "            update_mask = tf.where(pred_i[i] > 0, 0., mask_i[i])\n",
        "            update_peso_act = tf.where(pred_i[i] > 0, peso_act - pesos_i[i], peso_act)\n",
        "            #print(i)\n",
        "            return [update_peso_act, i + 1]\n",
        "\n",
        "        peso_act = tf.reduce_sum(pesos_i * pred_i)\n",
        "        #print(peso_act)\n",
        "        #print(target_0_i)\n",
        "        i = tf.constant(0)\n",
        "        # Bucle while en TensorFlow\n",
        "        peso_act, _ = tf.while_loop(cond, body, loop_vars=[peso_act, i])\n",
        "\n",
        "        final_loss_i = 1.0 / tf.reduce_sum(valores_i * mask_i * pred_i)\n",
        "        return final_loss_i\n",
        "\n",
        "    # Aplicar 'process_instance' a cada par (target_i, output_i) usando map_fn\n",
        "    final_losses = tf.map_fn(process_instance, (target, output), dtype=tf.float32)\n",
        "    return final_losses\n",
        "\n",
        "# Dummy data para ejemplificar la compilación\n",
        "num_items = 10  # Supongamos que cada instancia tiene 10 items\n",
        "batch_size = 32\n",
        "dummy_target = tf.convert_to_tensor(np.array(trainY[:batch_size], dtype=np.float64),dtype=tf.float32)\n",
        "dummy_output = tf.random.uniform([batch_size, 2 * num_items + 1],minval=0.99, maxval=1.0)\n",
        "#print(dummy_target[0], dummy_output[0])\n",
        "# Ejemplo de llamada a la función\n",
        "loss_values = fitness_loss_custom2(dummy_target, dummy_output)\n",
        "#print(loss_values)\n"
      ],
      "metadata": {
        "id": "el-PFg-kTWeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta función de pérdida `fitness_loss_custom3` es la función de pérdida finalmente usada comprobando que funcione como se espera."
      ],
      "metadata": {
        "id": "PgF2exn-Ma1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "def fitness_loss_custom3(target, output):\n",
        "    # Función que se aplicará a cada par (target_i, output_i)\n",
        "    def process_instance(args):\n",
        "        #print(args)\n",
        "        target_i, output_i = args\n",
        "        target_0_i = target_i[0]\n",
        "        valores_i = target_i[2::2]\n",
        "        pesos_i = target_i[1::2]\n",
        "        pred_i = output_i[int(output_i.shape[0] / 2)+1:]\n",
        "\n",
        "        mask_i = tf.ones_like(valores_i)\n",
        "        # Funciones para condiciones y cuerpo del bucle while\n",
        "        def cond(peso_act, i, mask_i):\n",
        "            return tf.reduce_any(peso_act > target_0_i)\n",
        "\n",
        "        def body(peso_act, i, mask_i):\n",
        "            update_mask = tf.where(pred_i[i] > 0, 0., mask_i[i])\n",
        "            update_peso_act = tf.where(pred_i[i] > 0, peso_act - pesos_i[i], peso_act)\n",
        "            #print(i)\n",
        "            return [update_peso_act, i + 1, tf.tensor_scatter_nd_update(mask_i, [[i]], [update_mask])]\n",
        "\n",
        "        peso_act = tf.reduce_sum(pesos_i * pred_i)\n",
        "        #print(peso_act)\n",
        "        #print(target_0_i)\n",
        "        # print(peso_act,valores_i,pred_i,mask_i)\n",
        "        i = tf.constant(0)\n",
        "        # Bucle while en TensorFlow\n",
        "        [peso_act, i ,  mask_i] = tf.while_loop(cond, body, loop_vars=[peso_act, i,mask_i])\n",
        "        # print(peso_act,mask_i)\n",
        "\n",
        "        final_loss_i = tf.reduce_sum(valores_i * mask_i * pred_i)\n",
        "        # print(final_loss_i)\n",
        "        return final_loss_i\n",
        "\n",
        "    # Aplicar 'process_instance' a cada par (target_i, output_i) usando map_fn\n",
        "    final_losses = tf.map_fn(process_instance, (target, output), fn_output_signature=tf.float32)\n",
        "\n",
        "    return 100.0 / tf.reduce_sum(final_losses)\n",
        "\n",
        "# Dummy data para ejemplificar la compilación\n",
        "num_items = 10  # Supongamos que cada instancia tiene 10 items\n",
        "batch_size = 32\n",
        "dummy_target = tf.convert_to_tensor(np.array(trainY[:batch_size], dtype=np.float64),dtype=tf.float32)\n",
        "dummy_output = tf.random.uniform([batch_size, 2 * num_items + 1],minval=0.99, maxval=1.0)\n",
        "#print(dummy_target[0], dummy_output[0])\n",
        "# Ejemplo de llamada a la función\n",
        "loss_values = fitness_loss_custom3(dummy_target, dummy_output)\n",
        "print(loss_values)\n",
        "bce = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "bce(dummy_target, dummy_output)"
      ],
      "metadata": {
        "id": "2x1Isua3Y8JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta funcion calcula exactamente el valor fitness de cada muestra por separado redondeando las probabilidades de la red neuronal para obtener la solucion que ofrece. Se ha observado que no funciona correctamente con la red neuronal debido al caracter probabilistico  y no entero de las redes neuronales para clasificación."
      ],
      "metadata": {
        "id": "9bGlfIjjCPH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def fitness_loss_custom_entera(target, output):\n",
        "    # Función que se aplicará a cada par (target_i, output_i)\n",
        "    def process_instance(args):\n",
        "        #print(args)\n",
        "        target_i, output_i = args\n",
        "        target_0_i = target_i[0]\n",
        "        valores_i = target_i[2::2]\n",
        "        pesos_i = target_i[1::2]\n",
        "        pred_i = output_i[int(output_i.shape[0] / 2)+1:]\n",
        "\n",
        "        #mask_i = tf.ones_like(valores_i)\n",
        "        mask_i = tf.where(pred_i < 0.5, 0., 1.)\n",
        "        # Funciones para condiciones y cuerpo del bucle while\n",
        "        def cond(peso_act, i, mask_i):\n",
        "            return tf.reduce_any(peso_act > target_0_i)\n",
        "\n",
        "        def body(peso_act, i, mask_i):\n",
        "            update_mask = tf.where(pred_i[i] > 0, 0., mask_i[i])\n",
        "            update_peso_act = tf.where(pred_i[i] > 0, peso_act - pesos_i[i], peso_act)\n",
        "            #print(i)\n",
        "            return [update_peso_act, i + 1, tf.tensor_scatter_nd_update(mask_i, [[i]], [update_mask])]\n",
        "\n",
        "        peso_act = tf.reduce_sum(pesos_i * mask_i)\n",
        "        #print(peso_act)\n",
        "        #print(target_0_i)\n",
        "        #print(peso_act,valores_i,pesos_i,pred_i,mask_i)\n",
        "        i = tf.constant(0)\n",
        "        # Bucle while en TensorFlow\n",
        "        [peso_act, i ,  mask_i] = tf.while_loop(cond, body, loop_vars=[peso_act, i,mask_i])\n",
        "        #print(peso_act,mask_i)\n",
        "\n",
        "        final_loss_i = tf.reduce_sum(valores_i * mask_i)\n",
        "        #print(final_loss_i)\n",
        "        return final_loss_i\n",
        "\n",
        "    # Aplicar 'process_instance' a cada par (target_i, output_i) usando map_fn\n",
        "    final_losses = tf.map_fn(process_instance, (target, output), fn_output_signature=tf.float32)\n",
        "\n",
        "    return 100.0 / tf.reduce_sum(final_losses)\n",
        "\n",
        "# Dummy data para ejemplificar la compilación\n",
        "num_items = 10  # Supongamos que cada instancia tiene 10 items\n",
        "batch_size = 32\n",
        "dummy_target = tf.convert_to_tensor(np.array(trainY[:batch_size], dtype=np.float64),dtype=tf.float32)\n",
        "dummy_output = tf.random.uniform([batch_size, 2 * num_items + 1],minval=0.0, maxval=1.0)\n",
        "#print(dummy_target[0], dummy_output[0])\n",
        "# Ejemplo de llamada a la función\n",
        "loss_values = fitness_loss_custom_entera(dummy_target, dummy_output)\n",
        "print(loss_values)\n",
        "bce = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "bce(dummy_target, dummy_output)"
      ],
      "metadata": {
        "id": "kesG8KMm3991"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del Modelo"
      ],
      "metadata": {
        "id": "PS13MmCRAP7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valores de entrada y salida de los modelos:"
      ],
      "metadata": {
        "id": "I40U7OA64U6T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPMym2XX5PCQ"
      },
      "outputs": [],
      "source": [
        "input_shape  = 2*numItems +1\n",
        "output_shape = input_shape  # Las etiquetas incluyen la instancia del problema\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a implementar una función que nos cree el modelo que vamos a entrenar y evaluar."
      ],
      "metadata": {
        "id": "kSJyJMzScEVm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZvUqrtH0sRw"
      },
      "outputs": [],
      "source": [
        "def create_model(dense_neurons, input_shape, output_shape):\n",
        "    local_model = keras.Sequential()\n",
        "\n",
        "    # Capa de entrada\n",
        "    local_model.add(layers.Input(shape=(int(input_shape),)))\n",
        "\n",
        "    # capas ocultas con activación Relu\n",
        "    for neurons in dense_neurons:\n",
        "      local_model.add(layers.Dense(neurons, activation='relu'))\n",
        "\n",
        "    # Capa de salida para clasificación con tantas neuronas como salidas esperadas\n",
        "    local_model.add(layers.Dense(output_shape, activation='sigmoid'))\n",
        "    return local_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para crear un modelo con dropout"
      ],
      "metadata": {
        "id": "vU6AsyMVND3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOEK0MO5BxkP"
      },
      "outputs": [],
      "source": [
        "def create_model_dropout(dense_neurons, input_shape, output_shape):\n",
        "    local_model = keras.Sequential()\n",
        "\n",
        "    # Capa de entrada\n",
        "    local_model.add(layers.Input(shape=(int(input_shape),)))\n",
        "\n",
        "    # capas ocultas con activación Relu\n",
        "    for neurons in dense_neurons:\n",
        "      local_model.add(layers.Dense(neurons, activation='relu'))\n",
        "      local_model.add(layers.Dropout(0.5))\n",
        "\n",
        "    # Capa de salida para clasificación con tantas neuronas como salidas esperadas\n",
        "    local_model.add(layers.Dense(output_shape, activation='sigmoid'))\n",
        "    return local_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para crear un modelo con normalización por lotes"
      ],
      "metadata": {
        "id": "nFayf582NJNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_normalization(dense_neurons, input_shape, output_shape):\n",
        "    local_model = keras.Sequential()\n",
        "\n",
        "    # Capa de entrada\n",
        "    local_model.add(layers.Input(shape=(int(input_shape),)))\n",
        "\n",
        "    # Capas ocultas con activación ReLU\n",
        "    for neurons in dense_neurons:\n",
        "        local_model.add(layers.Dense(neurons))\n",
        "        local_model.add(layers.BatchNormalization())  # Añade Batch Normalization\n",
        "        local_model.add(layers.Activation('relu'))    # Aplica la activación después de normalizar\n",
        "\n",
        "    # Capa de salida para clasificación con tantas neuronas como salidas esperadas\n",
        "    local_model.add(layers.Dense(output_shape, activation='sigmoid'))\n",
        "\n",
        "    return local_model"
      ],
      "metadata": {
        "id": "pEeoOJa1GvY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search"
      ],
      "metadata": {
        "id": "owAsBWNdMZSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la función para utilizar grid_search"
      ],
      "metadata": {
        "id": "k2WwQB60YNix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_grid_search(dense_neurons, input_shape, output_shape, X_scaled,X, Y, exact_sols, epochs, batch_size, learn_rate, _n_splits = 5, _random_state = 33):\n",
        "\n",
        "    # Inicializar KFold\n",
        "    kfold = KFold(n_splits=_n_splits, shuffle=True, random_state=_random_state)\n",
        "\n",
        "    loss_results = []\n",
        "    mejora_results = []\n",
        "    fitness_results = []\n",
        "\n",
        "    # Iterar sobre cada fold (validación cruzada)\n",
        "    for train_index, test_index in kfold.split(X_scaled):\n",
        "\n",
        "\n",
        "        # Dividir datos en entrenamiento y prueba\n",
        "        trainX_scaled, valX_scaled = X_scaled[train_index], X_scaled[test_index]\n",
        "        trainX, valX = X[train_index], X[test_index]\n",
        "        trainY, valY = Y[train_index], Y[test_index]\n",
        "\n",
        "        # Crear modelo\n",
        "        local_model = create_model(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "        # Compilar el modelo\n",
        "        sgd_optimizer = SGD(learning_rate=learn_rate)\n",
        "        local_model.compile(optimizer=sgd_optimizer,\n",
        "                            loss = fitness_loss_custom3)\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        local_model.fit(\n",
        "            trainX_scaled, trainY,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        # Evaluar el modelo\n",
        "        scores = local_model.evaluate(valX_scaled, valY, verbose=0)\n",
        "        loss_results.append(scores)\n",
        "\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = local_model.predict(valX_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_results.append(metrica_fitness_mejor_solucion(y_pred_binary, valX, exact_sols[test_index]))\n",
        "        fitness_results.append(metrica_valor_fitness(y_pred_binary, valX))\n",
        "\n",
        "    # Devolver el promedio de las evaluaciones\n",
        "    return np.mean(loss_results), np.std(loss_results),np.mean(mejora_results),np.mean(fitness_results)"
      ],
      "metadata": {
        "id": "sHKcTAuUaW-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valores de los hiperparámetros a probar:"
      ],
      "metadata": {
        "id": "wP6IcsRrcFU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores de los hiperparámetros para el modelo base\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "epochs = 40\n",
        "k_folds = 5\n",
        "\n",
        "\n",
        "# Valores de los  distintos hiperparámetros\n",
        "learning_rate_search  = [0.1,0.01,0.001,0.0001]\n",
        "batch_size_search = [16,32,64,128]"
      ],
      "metadata": {
        "id": "citY87iPFh51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hiperparámetro de la tasa de aprendizaje (learning rate)"
      ],
      "metadata": {
        "id": "M4I5H2MoYXy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_mean_table = []\n",
        "loss_std_table = []\n",
        "mejora_table = []\n",
        "fitness_table = []\n",
        "\n",
        "for lr in learning_rate_search:\n",
        "    # Entrenar y evaluar\n",
        "    results = single_grid_search(dense_neurons, input_shape, output_shape, trainX_scaled, trainX, trainY, datos[trainIndexes,-1], epochs, batch_size, lr, k_folds, random_state)\n",
        "\n",
        "    loss_mean_table.append(results[0])\n",
        "    loss_std_table.append(results[1])\n",
        "    mejora_table.append(results[2])\n",
        "    fitness_table.append(results[3])\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Tasa de Aprendizaje': learning_rate_search,'Error de validación medio (función de pérdida)':loss_mean_table,'Desviación del error de validación (función de pérdida)':loss_std_table,'Mejora':mejora_table,'Fitness':fitness_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "Zm15CIBrhS4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hiperparámetro del tamaño de lote (batch size)"
      ],
      "metadata": {
        "id": "Ce8eXwwnlMvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_mean_table = []\n",
        "loss_std_table = []\n",
        "mejora_table = []\n",
        "fitness_table = []\n",
        "\n",
        "for bs in batch_size_search:\n",
        "    # Entrenar y evaluar\n",
        "    results = single_grid_search(dense_neurons, input_shape, output_shape, trainX_scaled, trainX, trainY, datos[trainIndexes,-1], epochs, bs, learning_rate, k_folds, random_state)\n",
        "\n",
        "    loss_mean_table.append(results[0])\n",
        "    loss_std_table.append(results[1])\n",
        "    mejora_table.append(results[2])\n",
        "    fitness_table.append(results[3])\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Tasa de Aprendizaje': batch_size_search,'Error de validación medio (función de pérdida)':loss_mean_table,'Desviación del error de validación (función de pérdida)':loss_std_table,'Mejora':mejora_table,'Fitness':fitness_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "ojQOJnm7lKxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo Final"
      ],
      "metadata": {
        "id": "KFhlnoxYFMBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores de los hiperparámetros para el modelo final\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)\n",
        "batch_size = 64\n",
        "epochs = 40\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "\n",
        "final_model = create_model(dense_neurons, input_shape, output_shape)\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "final_model.compile(optimizer=sgd_optimizer,\n",
        "                    loss=fitness_loss_custom3)\n",
        "#print(final_model.summary())\n",
        "\n",
        "final_model.fit(\n",
        "    trainX_scaled, trainY,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        ")"
      ],
      "metadata": {
        "id": "L89W6fCigpN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados"
      ],
      "metadata": {
        "id": "ZCnR5HSCHEQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fitness_table = []\n",
        "mejora_table = []\n",
        "loss_mean_table = []\n",
        "\n",
        "# Evaluar el modelo\n",
        "scores = final_model.evaluate(testX_scaled, testY, verbose=0)\n",
        "loss_mean_table.append(scores)\n",
        "\n",
        "# Evaluar metricas personalizadas\n",
        "y_pred = final_model.predict(testX_scaled,verbose=0)\n",
        "\n",
        "umbral = 0.5\n",
        "y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "mejora_table.append(metrica_fitness_mejor_solucion(y_pred_binary, testX,  datos[testIndexes,-1]))\n",
        "fitness_table.append(metrica_valor_fitness(y_pred_binary, testX))\n",
        "\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Error de validación (función de pérdida)':loss_mean_table,'Mejora':mejora_table,'Fitness':fitness_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "f8XGcSO0nFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curvas de Aprendizaje Función de Pérdida Probabilistica"
      ],
      "metadata": {
        "id": "CX5uD1aUnFz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_curves_mejora(curves_model, train_X_scaled, train_X, train_Y, val_X_scaled, val_X, val_Y, exact_sols_train, exact_sols_val, batch_size,epochs, iters):\n",
        "    loss_results = [[],[]]\n",
        "    mejora_results = [[],[]]\n",
        "    fitness_results = [[],[]]\n",
        "\n",
        "    for i in range(iters):\n",
        "        # Entrenar el modelo\n",
        "        curves_model.fit(\n",
        "            train_X_scaled, train_Y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=2,\n",
        "        )\n",
        "\n",
        "        e_type = 0 # Entrenameinto\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(train_X_scaled, train_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(train_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, train_X, exact_sols_train))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, train_X))\n",
        "\n",
        "\n",
        "        e_type = 1 # Validación\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(val_X_scaled, val_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(val_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, val_X, exact_sols_val))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, val_X))\n",
        "\n",
        "    return loss_results, mejora_results, fitness_results,\n",
        "\n",
        "\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "curves_model = create_model(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "curves_model.compile(optimizer=sgd_optimizer,\n",
        "                    loss=fitness_loss_custom3)\n",
        "\n",
        "epochs = 1\n",
        "iters = 40\n",
        "curves_results = create_learning_curves_mejora(curves_model, trainX_scaled, trainX, trainY,testX_scaled,testX,testY,datos[trainIndexes,-1], datos[testIndexes,-1], batch_size, epochs,iters)"
      ],
      "metadata": {
        "id": "oZ51gOl8nFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de Pérdida"
      ],
      "metadata": {
        "id": "UuRW5cm4VT0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Función de Perdida\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Función de Pérdida\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[0][0], 'b',label=r'$Pérdida_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[0][1], 'orange',label=r'$Pérdida_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3byWIrmVT0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valor Fitness"
      ],
      "metadata": {
        "id": "a9Dx55KmVvN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(r\"Curva de Aprendizaje del Valor de $Fitness$\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $Fitness$\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[2][0], 'b',label=r'$Fitness_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[2][1], 'orange',label=r'$Fitness_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4yr1uJ6VvN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio de mejora soluciones exactas"
      ],
      "metadata": {
        "id": "SZQ1FjfEVHsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Mejora respecto de las Soluciones Exactas\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Ratio de Mejora\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[1][0], 'b',label=r'$Mejora_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[1][1], 'orange',label=r'$Mejora_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eaetWD48M0km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curvas de Aprendizaje Función de Pérdida Entera"
      ],
      "metadata": {
        "id": "xABdQ50oXzEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_curves_mejora(curves_model, train_X_scaled, train_X, train_Y, val_X_scaled, val_X, val_Y, exact_sols_train, exact_sols_val, batch_size,epochs, iters):\n",
        "    loss_results = [[],[]]\n",
        "    mejora_results = [[],[]]\n",
        "    fitness_results = [[],[]]\n",
        "\n",
        "    for i in range(iters):\n",
        "        # Entrenar el modelo\n",
        "        curves_model.fit(\n",
        "            train_X_scaled, train_Y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        e_type = 0 # Entrenameinto\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(train_X_scaled, train_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(train_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, train_X, exact_sols_train))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, train_X))\n",
        "\n",
        "\n",
        "        e_type = 1 # Validación\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(val_X_scaled, val_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(val_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, val_X, exact_sols_val))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, val_X))\n",
        "\n",
        "    return loss_results, mejora_results, fitness_results,\n",
        "\n",
        "\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "curves_model_entera = create_model(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "curves_model_entera.compile(optimizer=sgd_optimizer,\n",
        "                    loss=fitness_loss_custom_entera)\n",
        "\n",
        "epochs = 1\n",
        "iters = 40\n",
        "curves_results_entera = create_learning_curves_mejora(curves_model_entera, trainX_scaled, trainX, trainY,testX_scaled,testX,testY,datos[trainIndexes,-1], datos[testIndexes,-1], batch_size, epochs,iters)"
      ],
      "metadata": {
        "id": "6uAhexOJXzE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de Pérdida"
      ],
      "metadata": {
        "id": "l42Mv7YYXzE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Función de Perdida\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Función de Pérdida\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_entera[0][0], 'b',label=r'$Pérdida_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_entera[0][1], 'orange',label=r'$Pérdida_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QLmm1uSdXzE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valor Fitness"
      ],
      "metadata": {
        "id": "JvWXt7BMXzE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(r\"Curva de Aprendizaje del Valor de $Fitness$\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $Fitness$\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_entera[2][0], 'b',label=r'$Fitness{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_entera[2][1], 'orange',label=r'$Fitness{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a80805SWXzE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio de mejora soluciones exactas"
      ],
      "metadata": {
        "id": "xSpQfBR6XzE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Mejora respecto de las Soluciones Exactas\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Ratio de Mejora\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_entera[1][0], 'b',label=r'$Mejora_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_entera[1][1], 'orange',label=r'$Mejora_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kp_Jz_x7XzE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curvas de Aprendizaje Función de Pérdida Batch Normalization"
      ],
      "metadata": {
        "id": "lKOPtr-6na_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_curves_mejora(curves_model, train_X_scaled, train_X, train_Y, val_X_scaled, val_X, val_Y, exact_sols_train, exact_sols_val, batch_size,epochs, iters):\n",
        "    loss_results = [[],[]]\n",
        "    mejora_results = [[],[]]\n",
        "    fitness_results = [[],[]]\n",
        "\n",
        "    for i in range(iters):\n",
        "        # Entrenar el modelo\n",
        "        curves_model.fit(\n",
        "            train_X_scaled, train_Y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        e_type = 0 # Entrenameinto\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(train_X_scaled, train_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(train_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, train_X, exact_sols_train))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, train_X))\n",
        "\n",
        "\n",
        "        e_type = 1 # Validación\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(val_X_scaled, val_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(val_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, val_X, exact_sols_val))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, val_X))\n",
        "\n",
        "    return loss_results, mejora_results, fitness_results,\n",
        "\n",
        "\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "curves_model_norm = create_model_normalization(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "curves_model_norm.compile(optimizer=sgd_optimizer,\n",
        "                    loss=fitness_loss_custom3)\n",
        "\n",
        "print(curves_model_norm.summary())\n",
        "\n",
        "epochs = 1\n",
        "iters = 40\n",
        "curves_results_norm = create_learning_curves_mejora(curves_model_norm, trainX_scaled, trainX, trainY,testX_scaled,testX,testY,datos[trainIndexes,-1], datos[testIndexes,-1], batch_size, epochs,iters)"
      ],
      "metadata": {
        "id": "do6zRrgQna_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de Pérdida"
      ],
      "metadata": {
        "id": "YGsC7t8gna_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Función de Perdida\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Función de Pérdida\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_norm[0][0], 'b',label=r'$Pérdida_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_norm[0][1], 'orange',label=r'$Pérdida_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vSDL1Vf_na_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valor Fitness"
      ],
      "metadata": {
        "id": "Mwcqjmcana_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(r\"Curva de Aprendizaje del Valor de $Fitness$\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $Fitness$\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_norm[2][0], 'b',label=r'$Fitness_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_norm[2][1], 'orange',label=r'$Fitness_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YqeXSy47na_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio de mejora soluciones exactas"
      ],
      "metadata": {
        "id": "Fwk1htWqna_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Mejora respecto de las Soluciones Exactas\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Ratio de Mejora\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_norm[1][0], 'b',label=r'$Mejora_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_norm[1][1], 'orange',label=r'$Mejora_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aTr07YW1na_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curvas de Aprendizaje Función de Pérdida Dropout"
      ],
      "metadata": {
        "id": "kRXs4tZehioP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_curves_mejora(curves_model, train_X_scaled, train_X, train_Y, val_X_scaled, val_X, val_Y, exact_sols_train, exact_sols_val, batch_size,epochs, iters):\n",
        "    loss_results = [[],[]]\n",
        "    mejora_results = [[],[]]\n",
        "    fitness_results = [[],[]]\n",
        "\n",
        "    for i in range(iters):\n",
        "        # Entrenar el modelo\n",
        "        curves_model.fit(\n",
        "            train_X_scaled, train_Y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=2,\n",
        "        )\n",
        "\n",
        "        e_type = 0 # Entrenameinto\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(train_X_scaled, train_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(train_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, train_X, exact_sols_train))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, train_X))\n",
        "\n",
        "\n",
        "        e_type = 1 # Validación\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(val_X_scaled, val_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores)\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(val_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        mejora_results[e_type].append(metrica_fitness_mejor_solucion(y_pred_binary, val_X, exact_sols_val))\n",
        "        fitness_results[e_type].append(metrica_valor_fitness(y_pred_binary, val_X))\n",
        "\n",
        "    return loss_results, mejora_results, fitness_results,\n",
        "\n",
        "\n",
        "dense_neurons  = (numItems*numItems*numItems,numItems*numItems*numItems,numItems*numItems*5,numItems*numItems*2)\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "curves_model_dropout = create_model_dropout(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "curves_model_dropout.compile(optimizer=sgd_optimizer,\n",
        "                    loss=fitness_loss_custom3)\n",
        "\n",
        "print(curves_model_dropout.summary())\n",
        "\n",
        "epochs = 1\n",
        "iters = 40\n",
        "curves_results_dropout = create_learning_curves_mejora(curves_model_dropout, trainX_scaled, trainX, trainY,testX_scaled,testX,testY,datos[trainIndexes,-1], datos[testIndexes,-1], batch_size, epochs,iters)"
      ],
      "metadata": {
        "id": "O-58cEV5hioW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de Pérdida"
      ],
      "metadata": {
        "id": "RDY1oixvhioW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Función de Perdida\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Función de Pérdida\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_dropout[0][0], 'b',label=r'$Pérdida_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_dropout[0][1], 'orange',label=r'$Pérdida_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jpfh8_dihioW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valor Fitness"
      ],
      "metadata": {
        "id": "se117jZihioW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(r\"Curva de Aprendizaje del Valor de $Fitness$\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $Fitness$\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_dropout[2][0], 'b',label=r'$Fitness_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_dropout[2][1], 'orange',label=r'$Fitness_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BN6S-k6EhioW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio de mejora soluciones exactas"
      ],
      "metadata": {
        "id": "haHHVHiUhioX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Mejora respecto de las Soluciones Exactas\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Ratio de Mejora\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_dropout[1][0], 'b',label=r'$Mejora_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results_dropout[1][1], 'orange',label=r'$Mejora_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2) # poner la cuadricula con el grosor del resultado dado\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2wnkPfRohioX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}