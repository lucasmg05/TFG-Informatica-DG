{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KUH4RQKDnFz1",
        "FsN4FjUVnFz1",
        "L3z0D3XInFz1",
        "MTwKHXkTnFz2",
        "wlkLYvh8nFz3",
        "CX5uD1aUnFz5",
        "FtoeTMqk-LtW",
        "paEya3YZF7Yi",
        "uYCZqjVVX3wH",
        "eaBcBckpMVzt",
        "xMOrI2hx_bK4",
        "JKvRABs1XYAw",
        "xfErMhZ4ZRWJ",
        "I2ROBk7nUxBt",
        "6-x3OHfkJut3",
        "lc7Y3YplXUt-",
        "owAsBWNdMZSE",
        "ZCnR5HSCHEQY",
        "BJojw7eWtKeo",
        "VPTybl8USxK-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Inicialización"
      ],
      "metadata": {
        "id": "ZH6MfWwi7tSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "random_state = 33"
      ],
      "metadata": {
        "id": "HN4sfCSZwW4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Para acceder a los ficheros de Google Drive\n",
        "drive.mount('/content/drive')\n",
        "# La carpeta datos debe estar en vuestro Drive, dentro de la carpeta 'Colab Notebooks'"
      ],
      "metadata": {
        "id": "63VrcRrnwW4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experimento Mochila $N=20$ y $10^5$ casos"
      ],
      "metadata": {
        "id": "yV5hRq03nFzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido al alto coste computacional de entrenar las redes neuronales, ejecutar etse experimento lleva una cantidad considerable de horas."
      ],
      "metadata": {
        "id": "e6lOfpL4N3Ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El problema utiliza $N$ objetos exactamente y los datos se cargan desde un fichero .csv como un vector de X casos donde cada posición es un vector de $4N+2$ variables que representan:   \n",
        "\n",
        "\n",
        "- 0   Número de Objetos.  \n",
        "- 1   Capacidad de la mochila.   \n",
        "- 2 - 2N+1 Peso y Valor de cada objeto en ese orden.   \n",
        "- 2N+2 - 3N+1 Vector binario solucion peor.  \n",
        "- 3N+2 -4N+1 Vector binario de solucion mejor.\n",
        "\n"
      ],
      "metadata": {
        "id": "YGapVqWRnFz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar datos"
      ],
      "metadata": {
        "id": "KUH4RQKDnFz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'drive/MyDrive/Colab Notebooks/datos/TFG/data_E1_Mochila.csv'\n",
        "\n",
        "datos_Gr_FP = np.genfromtxt(file_path, delimiter=\",\") #Funcion para cargar datos desde txt, y en este caso desde csv\n",
        "\n",
        "numItems = datos_Gr_FP[0][0]\n",
        "x_Gr_FP = datos_Gr_FP[:, 1: int(3*numItems+2)]  # datos de entrada\n",
        "y_Gr_FP = datos_Gr_FP[:, int(3*numItems+2):]   # datos de etiqueta\n",
        "x_Gr_FP = np.array(x_Gr_FP, np.float64)\n",
        "y_Gr_FP = np.array(y_Gr_FP, np.float64)\n",
        "\n",
        "print(\"Número de objetos (N) =\", numItems)\n",
        "# Mostrar dimension del conjunto de muestras total\n",
        "print(\"Forma de vector X de muestras:\", x_Gr_FP.shape)\n",
        "print(\"Forma de vector Y de etiquetas:\", y_Gr_FP.shape)\n",
        "\n",
        "print(\"Ejemplos:\")\n",
        "print(\"X = \",x_Gr_FP[0])\n",
        "print(\"Y = \",y_Gr_FP[0])"
      ],
      "metadata": {
        "id": "W4DbyaUGnFz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separación de datos en Train y Test"
      ],
      "metadata": {
        "id": "FsN4FjUVnFz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separo el conjunto de casos en test y train para evaluar los modelos utilizados. También se pueden utilizar funciones como `sklearn.model_selection.train_test_split()` pero de esta forma se pueden guardar los índices para realizar comprobaciones de forma más sencilla."
      ],
      "metadata": {
        "id": "sjttGW8snFz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "trainPortion = 0.8 #porcentaje de train, el porcentaje de test será la resta de 1 menos el porcentaje de train\n",
        "\n",
        "#-------------Obtener índices\n",
        "\n",
        "indexesData = np.arange(len(y_Gr_FP)) #Indices del conjunto de muestras\n",
        "#-------------Desordenar indices y separar en rain y test\n",
        "\n",
        "np.random.shuffle(indexesData) #Desordenar indices de las muestras\n",
        "numberTrain = round(len(indexesData)*trainPortion) #numero de muestras para train\n",
        "trainIndexes = indexesData[:numberTrain]\n",
        "testIndexes = indexesData[numberTrain:]\n",
        "\n",
        "#-------------Datos desorden:ados para train y test\n",
        "\n",
        "trainX_Gr_FP = x_Gr_FP[trainIndexes]\n",
        "testX_Gr_FP = x_Gr_FP[testIndexes]\n",
        "trainY_Gr_FP = y_Gr_FP[trainIndexes]\n",
        "testY_Gr_FP = y_Gr_FP[testIndexes]\n",
        "\n",
        "#-------------Copia de los datos para mantener datos originales después del preprocesado\n",
        "\n",
        "original_trainX_Gr_FP = trainX_Gr_FP.copy()\n",
        "original_testX_Gr_FP = testX_Gr_FP.copy()\n",
        "original_trainY_Gr_FP = trainY_Gr_FP.copy()\n",
        "original_testY_Gr_FP = testY_Gr_FP.copy()\n",
        "\n",
        "#-------------Mostrar resultados\n",
        "\n",
        "print('Muestras totales:  {}'.format(len(trainY_Gr_FP)+len(testX_Gr_FP)))\n",
        "print('Muestras train:  {}'.format(len(trainY_Gr_FP)))\n",
        "print('Muestras test:  {}'.format(len(testX_Gr_FP)))\n"
      ],
      "metadata": {
        "id": "iULyYsJmnFz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento de los datos"
      ],
      "metadata": {
        "id": "L3z0D3XInFz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como los datos han sido generados artificialmente y no se han obtenido de fuentes externas, no es necesario realizar la búsqueda de datos faltantes o erroneos ni desbalanceo en las proporciones ya que los datos han sido generados teniendo en cuenta estos factores. La codificación de los datos también se ha tenido en cuenta en al generación de los datos."
      ],
      "metadata": {
        "id": "uUlboYixnFz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comprobación de Duplicados en los datos"
      ],
      "metadata": {
        "id": "VEXUPYkJnFz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobando duplicados en datos\n",
        "unique_rows= np.unique(datos_Gr_FP, axis=0)\n",
        "num_duplicates = datos_Gr_FP.shape[0] - unique_rows.shape[0]\n",
        "\n",
        "print(\"Número de filas duplicadas en los datos:\", num_duplicates)"
      ],
      "metadata": {
        "id": "aXwz5tZanFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Escalar Datos"
      ],
      "metadata": {
        "id": "yvBYJ82FnFz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el objeto MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Ajustar el escalador con los datos de entrenamiento y transformar tanto train como test\n",
        "trainX_Gr_FP_scaled = scaler.fit_transform(trainX_Gr_FP)\n",
        "testX_Gr_FP_scaled = scaler.transform(testX_Gr_FP)\n",
        "trainX_Gr_FP_scaled = np.array(trainX_Gr_FP_scaled)\n",
        "testX_Gr_FP_scaled = np.array(testX_Gr_FP_scaled)\n",
        "\n",
        "print(\"X train = \",trainX_Gr_FP_scaled[0])\n",
        "print(\"X test= \",testX_Gr_FP_scaled[0])"
      ],
      "metadata": {
        "id": "uLNewZb6nFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones y Métricas Fitness"
      ],
      "metadata": {
        "id": "MTwKHXkTnFz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valor_fitness(instancia, caso):\n",
        "  instancia_local = instancia.copy()\n",
        "  num_items = len(instancia)\n",
        "  valores = caso[2:int(num_items*2+2):2]\n",
        "  pesos = caso[1:int(num_items*2+1):2]\n",
        "  pesoAct = np.sum(pesos*instancia_local)\n",
        "  i=0\n",
        "  while(pesoAct > caso[0]): # mientras el peso sea mayor\n",
        "    if(instancia_local[i]):\n",
        "      pesoAct -= pesos[i]\n",
        "      instancia_local[i] = 0\n",
        "    i += 1\n",
        "  return np.sum(valores*instancia_local)\n"
      ],
      "metadata": {
        "id": "yIjU3DA1nFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrica_valor_fitness_entrada(y_true, y_pred, x):\n",
        "  mejora = 0\n",
        "  num_tot = len(x)\n",
        "  num_items = len(y_true[0])\n",
        "  umbral = 0.5\n",
        "  predictions_binary = (y_pred >= umbral).astype(int)\n",
        "  for i in range(num_tot):\n",
        "    valores = x[i][2:int(num_items*2+2):2]\n",
        "    instancia_entrada = x[i][int(num_items*2+1):int(num_items*3+2)]\n",
        "    valor_salida_pred = valor_fitness(predictions_binary[i], x[i])\n",
        "    valor_entrada = np.sum(instancia_entrada * valores)\n",
        "    mejora += valor_salida_pred / valor_entrada\n",
        "  return mejora/num_tot\n",
        "\n",
        "def metrica_valor_fitness_salida(y_true, y_pred, x):\n",
        "  mejora = 0\n",
        "  num_tot = len(x)\n",
        "  num_items = len(y_true[0])\n",
        "  umbral = 0.5\n",
        "  predictions_binary = (y_pred >= umbral).astype(int)\n",
        "  for i in range(num_tot):\n",
        "    valores = x[i][2:int(num_items*2+2):2]\n",
        "    valor_salida_real = np.sum(y_true[i] * valores)\n",
        "    valor_salida_pred = valor_fitness(predictions_binary[i], x[i])\n",
        "    mejora += valor_salida_pred / valor_salida_real\n",
        "  return mejora/num_tot"
      ],
      "metadata": {
        "id": "UfErg6bonFz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrica_porcentaje_mejor_entrada(y_true, y_pred, x):\n",
        "  num_aciertos = 0\n",
        "  num_tot = len(x)\n",
        "  num_items = len(y_true[0])\n",
        "  umbral = 0.5\n",
        "  predictions_binary = (y_pred >= umbral).astype(int)\n",
        "  for i in range(num_tot):\n",
        "    valores = x[i][2:int(num_items*2+2):2]\n",
        "    instancia_entrada = x[i][int(num_items*2+1):int(num_items*3+2)]\n",
        "    valor_salida_pred = valor_fitness(predictions_binary[i], x[i])\n",
        "    valor_entrada = np.sum(instancia_entrada * valores)\n",
        "    if valor_salida_pred > valor_entrada:\n",
        "      num_aciertos += 1\n",
        "  return num_aciertos/num_tot\n",
        "\n",
        "def metrica_porcentaje_mejor_salida(y_true, y_pred, x):\n",
        "  num_aciertos = 0\n",
        "  num_tot = len(x)\n",
        "  num_items = len(y_true[0])\n",
        "  umbral = 0.5\n",
        "  predictions_binary = (y_pred >= umbral).astype(int)\n",
        "  for i in range(num_tot):\n",
        "    valores = x[i][2:int(num_items*2+2):2]\n",
        "    valor_salida_real = np.sum(y_true[i] * valores)\n",
        "    valor_salida_pred = valor_fitness(predictions_binary[i], x[i])\n",
        "    if valor_salida_pred > valor_salida_real:\n",
        "      num_aciertos += 1\n",
        "  return num_aciertos/num_tot"
      ],
      "metadata": {
        "id": "Gg0QqxH5nFz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ratio de mejora de las soluciones FPTAS respecto de las soluciones aleatorias:\",metrica_valor_fitness_entrada(trainY_Gr_FP,trainY_Gr_FP,trainX_Gr_FP)*100,\"%\")\n",
        "print(\"Porcentaje de soluciones FPTAS mejores que las soluciones aleatorias:\",metrica_porcentaje_mejor_entrada(trainY_Gr_FP,trainY_Gr_FP,trainX_Gr_FP)*100,\"%\")"
      ],
      "metadata": {
        "id": "6P5ymUOAnFz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creación del Modelo"
      ],
      "metadata": {
        "id": "wlkLYvh8nFz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valores de entrada y salida de los modelos:"
      ],
      "metadata": {
        "id": "BXm5raoTnFz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iyf2kDaLnFz4"
      },
      "outputs": [],
      "source": [
        "input_shape  = 3*numItems +1\n",
        "output_shape = numItems  #  Numero de valores a predecir (la solucion al problema)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a implementar una función que nos cree el modelo que vamos a entrenar y evaluar."
      ],
      "metadata": {
        "id": "4q39VyzInFz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cDlAaVfnFz3"
      },
      "outputs": [],
      "source": [
        "def create_model(dense_neurons, input_shape, output_shape):\n",
        "    local_model = keras.Sequential()\n",
        "\n",
        "    # Capa de entrada\n",
        "    local_model.add(layers.Input(shape=(int(input_shape),)))\n",
        "\n",
        "    # capas ocultas con activación Relu\n",
        "    for neurons in dense_neurons:\n",
        "      local_model.add(layers.Dense(neurons, activation='relu'))\n",
        "\n",
        "    # Capa de salida para clasificación con tantas neuronas como salidas esperadas\n",
        "    local_model.add(layers.Dense(output_shape, activation='sigmoid'))\n",
        "    return local_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search"
      ],
      "metadata": {
        "id": "g9Jv_863nFz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la función para utilizar grid_search"
      ],
      "metadata": {
        "id": "-n9J3QEYnFz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def single_grid_search(dense_neurons, input_shape, output_shape, X_scaled,X, Y, epochs, batch_size, learn_rate, _n_splits = 5, _random_state = 33):\n",
        "\n",
        "    # Inicializar KFold\n",
        "    kfold = KFold(n_splits=_n_splits, shuffle=True, random_state=_random_state)\n",
        "\n",
        "    accuracy_results = []\n",
        "    loss_results = []\n",
        "    mejora_entrada_results = []\n",
        "    mejora_salida_results = []\n",
        "    porc_mejor_entrada_results = []\n",
        "    porc_mejor_salida_results = []\n",
        "\n",
        "    # Iterar sobre cada fold (validación cruzada)\n",
        "    for train_index, test_index in kfold.split(X_scaled):\n",
        "        # Dividir datos en entrenamiento y prueba\n",
        "        trainX_scaled, valX_scaled = X_scaled[train_index], X_scaled[test_index]\n",
        "        trainX, valX = X[train_index], X[test_index]\n",
        "        trainY, valY = Y[train_index], Y[test_index]\n",
        "\n",
        "        # Crear modelo\n",
        "        local_model = create_model(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "        # Compilar el modelo\n",
        "        sgd_optimizer = SGD(learning_rate=learn_rate)\n",
        "        local_model.compile(optimizer=sgd_optimizer,\n",
        "                            loss='binary_crossentropy',\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "        # Entrenar el modelo\n",
        "        local_model.fit(\n",
        "            trainX_scaled, trainY,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0,\n",
        "        )\n",
        "\n",
        "        # Evaluar el modelo\n",
        "        scores = local_model.evaluate(valX_scaled, valY, verbose=0)\n",
        "        loss_results.append(scores[0])\n",
        "        accuracy_results.append(scores[1])\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = local_model.predict(valX_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_entrada_results.append(metrica_valor_fitness_entrada(valY,y_pred_binary,valX))\n",
        "        mejora_salida_results.append(metrica_valor_fitness_salida(valY,y_pred_binary,valX))\n",
        "        porc_mejor_entrada_results.append(metrica_porcentaje_mejor_entrada(valY,y_pred_binary,valX))\n",
        "        porc_mejor_salida_results.append(metrica_porcentaje_mejor_salida(valY,y_pred_binary,valX))\n",
        "\n",
        "    # Devolver el promedio de las evaluaciones\n",
        "    return np.mean(loss_results), np.mean(accuracy_results),np.mean(mejora_entrada_results),np.mean(mejora_salida_results),np.mean(porc_mejor_entrada_results),np.mean(porc_mejor_salida_results), loss_results"
      ],
      "metadata": {
        "id": "fbMOzGfcnFz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valores de los hiperparámetros a probar:"
      ],
      "metadata": {
        "id": "vD60Jt6AnFz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores de los hiperparámetros para el modelo base\n",
        "dense_neurons  = (numItems*numItems,numItems*numItems,numItems*numItems/2,numItems*2)\n",
        "epochs = 100\n",
        "learning_rate = 0.1\n",
        "batch_size = 64\n",
        "k_folds = 2\n",
        "\n",
        "# Valores de los  distintos hiperparámetros\n",
        "learning_rate_search  = [0.1,0.01,0.001,0.0001,0.00001]\n",
        "batch_size_search = [16,32,64,96,128]"
      ],
      "metadata": {
        "id": "ZShgdHVcnFz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hiperparámetro de la tasa de aprendizaje (learning rate)"
      ],
      "metadata": {
        "id": "z8Sc8km5nFz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_mean_table = []\n",
        "loss_mean_table = []\n",
        "mejora_entrada_table = []\n",
        "mejora_salida_table = []\n",
        "porc_mejor_entrada_table = []\n",
        "porc_mejor_salida_table = []\n",
        "\n",
        "for lr in learning_rate_search:\n",
        "    # Entrenar y evaluar\n",
        "    results = single_grid_search(dense_neurons, input_shape, output_shape, trainX_Gr_FP_scaled, trainX_Gr_FP, trainY_Gr_FP, epochs, batch_size, lr, k_folds, random_state)\n",
        "\n",
        "    loss_mean_table.append(results[0])\n",
        "    accuracy_mean_table.append(results[1])\n",
        "    mejora_entrada_table.append(results[2])\n",
        "    mejora_salida_table.append(results[3])\n",
        "    porc_mejor_entrada_table.append(results[4])\n",
        "    porc_mejor_salida_table.append(results[5])\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Tasa de Aprendizaje': learning_rate_search,'Error de validación(función de pérdida)':loss_mean_table,'Accuracy':accuracy_mean_table,'Mejora Solucion Incial':mejora_entrada_table,\n",
        "                           'Mejora Solucion Etiqueta':mejora_salida_table,'Porc Mejores que Inicial':porc_mejor_entrada_table, 'Porc Mejores que Etiqueta':porc_mejor_salida_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "NS-EJjqrnFz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Hiperparámetro del tamaño de lote (batch size)"
      ],
      "metadata": {
        "id": "beXyRY82nFz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_mean_table = []\n",
        "loss_mean_table = []\n",
        "mejora_entrada_table = []\n",
        "mejora_salida_table = []\n",
        "porc_mejor_entrada_table = []\n",
        "porc_mejor_salida_table = []\n",
        "\n",
        "for bs in batch_size_search:\n",
        "    # Entrenar y evaluar\n",
        "    results = single_grid_search(dense_neurons, input_shape, output_shape, trainX_Gr_FP_scaled, trainX_Gr_FP, trainY_Gr_FP, epochs, bs, learning_rate, k_folds, random_state)\n",
        "\n",
        "    loss_mean_table.append(results[0])\n",
        "    accuracy_mean_table.append(results[1])\n",
        "    mejora_entrada_table.append(results[2])\n",
        "    mejora_salida_table.append(results[3])\n",
        "    porc_mejor_entrada_table.append(results[4])\n",
        "    porc_mejor_salida_table.append(results[5])\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Tamaño de Lote': batch_size_search,'Error de validación(función de pérdida)':loss_mean_table,'Accuracy':accuracy_mean_table,'Mejora Solucion Incial':mejora_entrada_table,\n",
        "                           'Mejora Solucion Etiqueta':mejora_salida_table,'Porc Mejores que Inicial':porc_mejor_entrada_table, 'Porc Mejores que Etiqueta':porc_mejor_salida_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "ndTHDgq3nFz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo Final"
      ],
      "metadata": {
        "id": "kq7udHMFnFz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valores de los hiperparámetros para el modelo final\n",
        "dense_neurons  = (numItems*numItems,numItems*numItems,numItems*numItems/2,numItems*2)\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "final_model = create_model(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "final_model.compile(optimizer=sgd_optimizer,\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "final_model.fit(\n",
        "    trainX_Gr_FP_scaled, trainY_Gr_FP,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=0,\n",
        ")"
      ],
      "metadata": {
        "id": "NaOI3X4fnFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resultados"
      ],
      "metadata": {
        "id": "IXIOa-b3nFz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_mean_table = []\n",
        "mejora_entrada_table = []\n",
        "mejora_salida_table = []\n",
        "porc_mejor_entrada_table = []\n",
        "porc_mejor_salida_table = []\n",
        "loss_mean_table = []\n",
        "\n",
        "# Evaluar el modelo\n",
        "scores = final_model.evaluate(testX_Gr_FP_scaled, testY_Gr_FP, verbose=0)\n",
        "loss_mean_table.append(scores[0])\n",
        "accuracy_mean_table.append(scores[1])\n",
        "\n",
        "# Evaluar metricas personalizadas\n",
        "y_pred = final_model.predict(testX_Gr_FP_scaled,verbose=0)\n",
        "umbral = 0.5\n",
        "y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "mejora_entrada_table.append(metrica_valor_fitness_entrada(testY_Gr_FP, y_pred_binary, testX_Gr_FP))\n",
        "mejora_salida_table.append(metrica_valor_fitness_salida(testY_Gr_FP, y_pred_binary, testX_Gr_FP))\n",
        "porc_mejor_entrada_table.append(metrica_porcentaje_mejor_entrada(testY_Gr_FP, y_pred_binary, testX_Gr_FP))\n",
        "porc_mejor_salida_table.append(metrica_porcentaje_mejor_salida(testY_Gr_FP, y_pred_binary, testX_Gr_FP))\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Error de test (función de pérdida)':loss_mean_table,'Accuracy':accuracy_mean_table,'Mejora Solucion Incial':mejora_entrada_table,\n",
        "                           'Mejora Solucion Etiqueta':mejora_salida_table,'Porc Mejores que Inicial':porc_mejor_entrada_table, 'Porc Mejores que Etiqueta':porc_mejor_salida_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "xQRiSWhvnFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curvas de Aprendizaje"
      ],
      "metadata": {
        "id": "CX5uD1aUnFz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_curves_mejora(curves_model, train_X_scaled, train_X, train_Y, val_X_scaled, val_X, val_Y, batch_size,epochs, iters):\n",
        "    accuracy_results = [[],[]]\n",
        "    loss_results = [[],[]]\n",
        "    mejora_entrada_results = [[],[]]\n",
        "    mejora_salida_results = [[],[]]\n",
        "    porc_mejor_entrada_results = [[],[]]\n",
        "    porc_mejor_salida_results = [[],[]]\n",
        "\n",
        "    for i in range(iters):\n",
        "        # Entrenar el modelo\n",
        "        curves_model.fit(\n",
        "            train_X_scaled, train_Y,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=2,\n",
        "        )\n",
        "\n",
        "        e_type = 0 # Entrenameinto\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(train_X_scaled, train_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores[0])\n",
        "        accuracy_results[e_type].append(scores[1])\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(train_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_entrada_results[e_type].append(metrica_valor_fitness_entrada(train_Y,y_pred_binary,train_X))\n",
        "        mejora_salida_results[e_type].append(metrica_valor_fitness_salida(train_Y,y_pred_binary,train_X))\n",
        "        porc_mejor_entrada_results[e_type].append(metrica_porcentaje_mejor_entrada(train_Y,y_pred_binary,train_X))\n",
        "        porc_mejor_salida_results[e_type].append(metrica_porcentaje_mejor_salida(train_Y,y_pred_binary,train_X))\n",
        "\n",
        "\n",
        "        e_type = 1 # Validación\n",
        "        # Evaluar el modelo\n",
        "        scores = curves_model.evaluate(val_X_scaled, val_Y, verbose=0)\n",
        "        loss_results[e_type].append(scores[0])\n",
        "        accuracy_results[e_type].append(scores[1])\n",
        "\n",
        "        # Evaluar metricas personalizadas\n",
        "        y_pred = curves_model.predict(val_X_scaled,verbose=0)\n",
        "        umbral = 0.5\n",
        "        y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "\n",
        "        mejora_entrada_results[e_type].append(metrica_valor_fitness_entrada(val_Y,y_pred_binary,val_X))\n",
        "        mejora_salida_results[e_type].append(metrica_valor_fitness_salida(val_Y,y_pred_binary,val_X))\n",
        "        porc_mejor_entrada_results[e_type].append(metrica_porcentaje_mejor_entrada(val_Y,y_pred_binary,val_X))\n",
        "        porc_mejor_salida_results[e_type].append(metrica_porcentaje_mejor_salida(val_Y,y_pred_binary,val_X))\n",
        "\n",
        "    return accuracy_results, loss_results, mejora_entrada_results, mejora_salida_results, porc_mejor_entrada_results, porc_mejor_salida_results\n",
        "\n",
        "\n",
        "dense_neurons  = (numItems*numItems,numItems*numItems,numItems*numItems/2,numItems*2)\n",
        "batch_size = 64\n",
        "learning_rate = 0.1\n",
        "curves_model = create_model(dense_neurons, input_shape, output_shape)\n",
        "\n",
        "# Compilar el modelo\n",
        "sgd_optimizer = SGD(learning_rate=learning_rate)\n",
        "curves_model.compile(optimizer=sgd_optimizer,\n",
        "                    loss='binary_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "epochs = 5\n",
        "iters = 20\n",
        "curves_results = create_learning_curves_mejora(curves_model, trainX_Gr_FP_scaled, trainX_Gr_FP, trainY_Gr_FP,testX_Gr_FP_scaled,testX_Gr_FP,testY_Gr_FP, batch_size, epochs,iters)"
      ],
      "metadata": {
        "id": "oZ51gOl8nFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_mean_table = []\n",
        "mejora_entrada_table = []\n",
        "mejora_salida_table = []\n",
        "porc_mejor_entrada_table = []\n",
        "porc_mejor_salida_table = []\n",
        "loss_mean_table = []\n",
        "\n",
        "# Evaluar el modelo\n",
        "scores = curves_model.evaluate(testX_Gr_FP_scaled, testY_Gr_FP, verbose=0)\n",
        "loss_mean_table.append(scores[0])\n",
        "accuracy_mean_table.append(scores[1])\n",
        "\n",
        "# Evaluar metricas personalizadas\n",
        "y_pred = curves_model.predict(testX_Gr_FP_scaled,verbose=0)\n",
        "\n",
        "umbral = 0.5\n",
        "y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "mejora_entrada_table.append(metrica_valor_fitness_entrada(testY_Gr_FP,y_pred_binary,testX_Gr_FP))\n",
        "mejora_salida_table.append(metrica_valor_fitness_salida(testY_Gr_FP,y_pred_binary,testX_Gr_FP))\n",
        "porc_mejor_entrada_table.append(metrica_porcentaje_mejor_entrada(testY_Gr_FP,y_pred_binary,testX_Gr_FP))\n",
        "porc_mejor_salida_table.append(metrica_porcentaje_mejor_salida(testY_Gr_FP,y_pred_binary,testX_Gr_FP))\n",
        "\n",
        "\n",
        "#Crear DataFrame y mostrarlo\n",
        "tableFrame = pd.DataFrame({'Error de validación medio (función de pérdida)':loss_mean_table,'Accuracy medio':accuracy_mean_table,'Mejora Solucion Incial':mejora_entrada_table,\n",
        "                           'Mejora Solucion Etiqueta':mejora_salida_table,'Porc Mejores que Inicial':porc_mejor_entrada_table, 'Porc Mejores que Etiqueta':porc_mejor_salida_table})\n",
        "display(tableFrame)"
      ],
      "metadata": {
        "id": "f8XGcSO0nFz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función de Pérdida"
      ],
      "metadata": {
        "id": "UuRW5cm4VT0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Función de Pérdida\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Función de Pérdida\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[1][0], 'b',label=r'$Pérdida_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[1][1], 'orange',label=r'$Pérdida_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3byWIrmVT0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "a9Dx55KmVvN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(r\"Curva de Aprendizaje del $Accuracy$\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Accuracy\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[0][0], 'b',label=r'$Accuracy_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[0][1], 'orange',label=r'$Accuracy_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z4yr1uJ6VvN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio de mejora soluciones iniciales"
      ],
      "metadata": {
        "id": "SZQ1FjfEVHsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Mejora respecto de las Soluciones Iniciales\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Ratio de Mejora\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[2][0], 'b',label=r'$Mejora_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[2][1], 'orange',label=r'$Mejora_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eaetWD48M0km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratio de mejora soluciones Etiquetas"
      ],
      "metadata": {
        "id": "hc1ZCIsGV_s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje de la Mejora respecto de las Etiquetas\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Ratio de Mejora\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[3][0], 'b',label=r'$Mejora_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), curves_results[3][1], 'orange',label=r'$Mejora_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DeluIPk7V_tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Porcentaje de soluciones mejores que iniciales"
      ],
      "metadata": {
        "id": "KgfwHBApWAjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje del Porcentaje de Mejores soluciones respecto de las Iniciales\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Porcentaje de Mejores soluciones\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), np.array(curves_results[4][0])*100, 'b',label=r'$Porcentaje_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), np.array(curves_results[4][1])*100, 'orange',label=r'$Porcentaje_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JWKvcHJSWAjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Porcentaje de soluciones mejores que Etiquetas"
      ],
      "metadata": {
        "id": "kJ-rK7DAXGkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "plt.title(\"Curva de Aprendizaje del Porcentaje de Mejores soluciones respecto de las Etiquetas\",fontsize = 15)\n",
        "plt.xlabel(\"Época\",fontsize = 15)\n",
        "plt.ylabel(\"Porcentaje de Mejores soluciones\",fontsize = 15)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), np.array(curves_results[5][0])*100, 'b',label=r'$Porcentaje_{in}$', linewidth=2)\n",
        "plt.plot(range(epochs, epochs*(iters+1),epochs), np.array(curves_results[5][1])*100, 'orange',label=r'$Porcentaje_{out}$', linewidth=2)\n",
        "\n",
        "plt.legend(fontsize = 15)\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "opKSDi2_XGkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo Genético"
      ],
      "metadata": {
        "id": "pqRe8dVvKVws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funciones de adaptación"
      ],
      "metadata": {
        "id": "TFvcbeevQKbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformar_instancia_mochila_directo(pesos_valores, peso_maximo):\n",
        "    # Crear una lista que comienza con el peso máximo\n",
        "    vector_transformado = [peso_maximo]\n",
        "\n",
        "    # Añadir los pesos y valores alternados\n",
        "    for peso, valor in pesos_valores:\n",
        "        vector_transformado.append(peso)\n",
        "        vector_transformado.append(valor)\n",
        "\n",
        "    return vector_transformado\n",
        "\n",
        "\n",
        "def transformar_instancia_mochila_inverso(vector_transformado):\n",
        "    # Extraer el peso máximo, que es el primer elemento\n",
        "    peso_maximo = vector_transformado[0]\n",
        "\n",
        "    # Extraer los pesos y valores restantes\n",
        "    pesos_valores = []\n",
        "    # Iterar sobre el vector desde el segundo elemento hasta el final, tomando de dos en dos elementos\n",
        "    for i in range(1, len(vector_transformado), 2):\n",
        "        peso = vector_transformado[i]\n",
        "        valor = vector_transformado[i + 1]\n",
        "        pesos_valores.append([peso, valor])\n",
        "\n",
        "    return peso_maximo, pesos_valores\n",
        "\n",
        "\n",
        "def transformar_instancia_mochila_directo_con_sol(pesos_valores, peso_maximo,sol):\n",
        "    # Crear una lista que comienza con el peso máximo\n",
        "    vector_transformado = [peso_maximo]\n",
        "\n",
        "    # Añadir los pesos y valores alternados\n",
        "    for peso, valor in pesos_valores:\n",
        "        vector_transformado.append(peso)\n",
        "        vector_transformado.append(valor)\n",
        "    for aux in sol:\n",
        "        vector_transformado.append(aux)\n",
        "\n",
        "    return vector_transformado\n",
        "\n",
        "\n",
        "def concatenar_solucion_instancia(instancia,sol):\n",
        "    # Crear una lista que comienza con el peso máximo\n",
        "    vector_transformado = []\n",
        "\n",
        "    # Añadir los pesos y valores alternados\n",
        "    for aux in instancia:\n",
        "        vector_transformado.append(aux)\n",
        "\n",
        "    for aux in sol:\n",
        "        vector_transformado.append(aux)\n",
        "\n",
        "    return vector_transformado\n",
        "\n",
        "\n",
        "def transformar_soluciones_no_factibles(sol, peso_max, pesos_valores):\n",
        "  instancia_local = sol.copy()\n",
        "  instancia_locacl = np.array(instancia_local)\n",
        "  num_items = len(instancia_local)\n",
        "  pesos = np.array([item[0] for item in pesos_valores])\n",
        "  valores = np.array([item[1] for item in pesos_valores])\n",
        "  #print(pesos,instancia_local)\n",
        "  pesoAct = np.sum(pesos*instancia_local)\n",
        "  i=0\n",
        "  while(pesoAct > peso_max): # mientras el peso sea mayor\n",
        "    if(instancia_local[i] == 1):\n",
        "      pesoAct -= pesos[i]\n",
        "      instancia_local[i] = 0\n",
        "    i += 1\n",
        "  #print(pesoAct, np.sum(pesos*instancia_local),peso_max)\n",
        "  return instancia_local\n"
      ],
      "metadata": {
        "id": "H1QhDQgFU6FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algoritmo Genético simple"
      ],
      "metadata": {
        "id": "I9ogLQNEuNe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from random import getrandbits, randint, choice\n",
        "\n",
        "def fitness(sol, peso_max, pesos_valores):\n",
        "    peso_total, valor_total = 0, 0\n",
        "    for indice, valor in enumerate(sol):\n",
        "        peso_total += (sol[indice] * pesos_valores[indice][0])\n",
        "        valor_total += (sol[indice] * pesos_valores[indice][1])\n",
        "\n",
        "    if (peso_max - peso_total) >= 0:\n",
        "        return valor_total\n",
        "    else: # Se descartan las soluciones no factibles\n",
        "        return -1\n",
        "\n",
        "def media_fitness(poblacion, peso_max, pesos_valores):\n",
        "    sum_tot = sum(fitness(x, peso_max, pesos_valores) for x in poblacion if fitness(x, peso_max, pesos_valores) >= 0)\n",
        "    return sum_tot / (len(poblacion) * 1.0)\n",
        "\n",
        "\n",
        "def max_fitness(poblacion, peso_max, pesos_valores):\n",
        "    valores_fitness = [fitness(x, peso_max, pesos_valores) for x in poblacion if fitness(x, peso_max, pesos_valores) >= 0]\n",
        "    return max(valores_fitness) if valores_fitness else None\n",
        "\n",
        "def generar_sol(numItems):\n",
        "    return [ getrandbits(1) for x in range(numItems) ]\n",
        "\n",
        "def generar_pobl(num_sols, numItems):\n",
        "    return [ generar_sol(numItems) for x in range(num_sols) ]\n",
        "\n",
        "\n",
        "def elegir_antecesor(valores, fitness_total, indice_a_ignorar): # Metodo de ruleta\n",
        "    acumulado = 0\n",
        "    valor_radnom = random.random()\n",
        "\n",
        "    if indice_a_ignorar != -1:\n",
        "        fitness_total -= valores[0][indice_a_ignorar]\n",
        "\n",
        "    for indice, i in enumerate(valores[0]):\n",
        "        if indice_a_ignorar == indice:\n",
        "            continue\n",
        "        acumulado += i\n",
        "        if acumulado / fitness_total >= valor_radnom:\n",
        "            return indice\n",
        "\n",
        "def cruzar(soluciones):\n",
        "    valores = list(zip(*soluciones))\n",
        "    fitness_total = sum(valores[0])\n",
        "    indice_antecesor1 = elegir_antecesor(valores, fitness_total, -1)\n",
        "    indice_antecesor2 = elegir_antecesor(valores, fitness_total, indice_antecesor1)\n",
        "\n",
        "    antecesor1 = valores[1][indice_antecesor1]\n",
        "    antecesor2 = valores[1][indice_antecesor2]\n",
        "\n",
        "    return antecesor1, antecesor2\n",
        "\n",
        "def op_genetico(poblacion, peso_max, pesos_valores, num_sols_total, mut_prob=0.05):\n",
        "    soluciones = [ [fitness(x, peso_max, pesos_valores), x] for x in poblacion if fitness(x, peso_max, pesos_valores) >= 0]\n",
        "    soluciones.sort(reverse=True)\n",
        "\n",
        "\n",
        "    nuevas_sols = []\n",
        "    while len(nuevas_sols) < num_sols_total:\n",
        "        antecesor1, antecesor2 = cruzar(soluciones)\n",
        "        indice_mit = len(antecesor1) // 2\n",
        "        nueva_sol = antecesor1[:indice_mit] + antecesor2[indice_mit:]\n",
        "        nuevas_sols.append(nueva_sol)\n",
        "\n",
        "\n",
        "    for sol_act in nuevas_sols:\n",
        "        if mut_prob > random.random():\n",
        "            pos_to_mutate = randint(0, len(sol_act)-1)\n",
        "            if sol_act[pos_to_mutate] == 1:\n",
        "                sol_act[pos_to_mutate] = 0\n",
        "            else:\n",
        "                sol_act[pos_to_mutate] = 1\n",
        "\n",
        "    return nuevas_sols\n"
      ],
      "metadata": {
        "id": "5ShOxV4SKZX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1)\n",
        "\n",
        "peso_max, pesos_valores = transformar_instancia_mochila_inverso(trainX_Gr_FP[0][:int(numItems*2+1)])\n",
        "peso_max = int(peso_max)\n",
        "print(peso_max,pesos_valores)\n",
        "\n",
        "num_sols_total = 150\n",
        "iters = 200\n",
        "numItems = len(pesos_valores)\n",
        "#print(numItems)\n",
        "\n",
        "poblacion = generar_pobl(num_sols_total, numItems)\n",
        "poblacion = [transformar_soluciones_no_factibles(pop,peso_max,pesos_valores) for pop in poblacion]\n",
        "\n",
        "\n",
        "valores_medios = [media_fitness(poblacion, peso_max, pesos_valores)]\n",
        "valores_max = [max_fitness(poblacion, peso_max, pesos_valores)]\n",
        "for i in range(iters):\n",
        "    poblacion = op_genetico(poblacion, peso_max, pesos_valores, num_sols_total)\n",
        "    valores_medios.append(media_fitness(poblacion, peso_max, pesos_valores))\n",
        "    valores_max.append(max_fitness(poblacion,peso_max , pesos_valores))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ny44Vdx6ux7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(r\"Media del Valor $Fitness$ de la Población de Soluciones sin Red Neuronal\",fontsize = 15)\n",
        "plt.xlabel(\"Iteración\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $fitness$ medio\",fontsize = 15)\n",
        "plt.ylim( 100000 , 450000)\n",
        "plt.plot(range(iters+1), valores_medios,'b',label=r'$Fitness$ medio', linewidth=2)\n",
        "plt.legend(fontsize = 15,loc = 'lower right')\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(r\"Valor $Fitness$ Máximo de la Población de Soluciones sin Red Neuronal\",fontsize = 15)\n",
        "plt.xlabel(\"Iteración\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $fitness$ máximo\",fontsize = 15)\n",
        "plt.plot(range(iters+1), valores_max,'b',label=r'$Fitness$ máximo', linewidth=2)\n",
        "plt.ylim( 200000 , 500000)\n",
        "\n",
        "\n",
        "\n",
        "plt.legend(fontsize = 15,loc = 'lower right')\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XqeFSyGg6Y1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algoritmo Genético con red neuornal"
      ],
      "metadata": {
        "id": "QB4Fv3Sjupur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(1)\n",
        "\n",
        "peso_max, pesos_valores = transformar_instancia_mochila_inverso(trainX_Gr_FP[0][:int(numItems*2+1)])\n",
        "peso_max = int(peso_max)\n",
        "print(peso_max,pesos_valores)\n",
        "\n",
        "num_sols_total = 150\n",
        "iters = 200\n",
        "numItems = len(pesos_valores)\n",
        "\n",
        "poblacion = generar_pobl(num_sols_total, numItems)\n",
        "poblacion = [transformar_soluciones_no_factibles(pop,peso_max,pesos_valores) for pop in poblacion]\n",
        "\n",
        "\n",
        "eps = 0.05\n",
        "atasco = 0\n",
        "red_prob = 0.1\n",
        "\n",
        "\n",
        "valores_medios = [media_fitness(poblacion, peso_max, pesos_valores)]\n",
        "valores_max = [max_fitness(poblacion, peso_max, pesos_valores)]\n",
        "iters_modelo = []\n",
        "print(valores_medios)\n",
        "for i in range(iters):\n",
        "    poblacion = op_genetico(poblacion, peso_max, pesos_valores, num_sols_total)\n",
        "    valores_medios.append(media_fitness(poblacion, peso_max, pesos_valores))\n",
        "    valores_max.append(max_fitness(poblacion, peso_max, pesos_valores))\n",
        "    if valores_medios[-1] < (valores_medios[-2]*(1+eps)) :\n",
        "        atasco += 1\n",
        "        if atasco >= 20:\n",
        "          iters_modelo.append(i)\n",
        "          atasco = 0\n",
        "          for sol_act in poblacion:\n",
        "           if red_prob > random.random():\n",
        "              y_pred = final_model.predict(np.array([concatenar_solucion_instancia(trainX_Gr_FP_scaled[0][:int(numItems*2+1)], sol_act)]),verbose=0)\n",
        "              umbral = 0.5\n",
        "              y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "              sol_fact = transformar_soluciones_no_factibles(y_pred_binary[0], peso_max, pesos_valores)\n",
        "              sol_act = sol_fact.tolist()\n",
        "    else: atasco = 0"
      ],
      "metadata": {
        "id": "uOTFfQf3upus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(r\"Media del Valor $Fitness$ de la Población de Soluciones con Red Neuronal\",fontsize = 15)\n",
        "plt.xlabel(\"Iteración\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $fitness$ medio\",fontsize = 15)\n",
        "plt.plot(range(iters+1), valores_medios,'g',label=r'$Fitness$ Medio', linewidth=2)\n",
        "plt.scatter(iters_modelo, [valores_medios[i] for i in iters_modelo], color='purple', label='Llamada a la Red Neuronal', s=100, zorder=5)\n",
        "plt.ylim( 100000 , 450000)\n",
        "plt.legend(fontsize = 15,loc = 'lower right')\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(r\"Valor $Fitness$ Máximo de la Población de Soluciones con Red Neuronal\",fontsize = 15)\n",
        "plt.xlabel(\"Iteración\",fontsize = 15)\n",
        "plt.ylabel(r\"Valor $fitness$ máximo\",fontsize = 15)\n",
        "plt.plot(range(iters+1), valores_max,'g',label=r'$Fitness$ máximo', linewidth=2)\n",
        "plt.scatter(iters_modelo, [valores_max[i] for i in iters_modelo], color='purple', label='Llamada a la Red Neuronal', s=100, zorder=5)\n",
        "plt.ylim( 200000 , 500000)\n",
        "\n",
        "plt.legend(fontsize = 15, loc = 'lower right')\n",
        "plt.grid(visible=True,linewidth=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xq588gjG45dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ejemplos de Soluciones Generadas"
      ],
      "metadata": {
        "id": "paEya3YZF7Yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplos de soluciones distintas generadas por la red para la misma instancia del problema pero con diferentes soluciones iniciales:"
      ],
      "metadata": {
        "id": "iMDxJ3aF653h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sol_ini = transformar_soluciones_no_factibles(trainX_Gr_FP[0][-20:], peso_max, pesos_valores)\n",
        "y_pred = final_model.predict(np.array([concatenar_solucion_instancia(trainX_Gr_FP_scaled[0][:int(numItems*2+1)],sol_ini )]),verbose=0)\n",
        "umbral = 0.5\n",
        "y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "sol_fact = transformar_soluciones_no_factibles(y_pred_binary[0], peso_max, pesos_valores)\n",
        "sol_act = sol_fact.tolist()\n",
        "print(\"Solución Inicial:\",sol_ini, \"Valor fitness\", fitness(sol_ini, peso_max, pesos_valores))\n",
        "print(\"Solución generada\",sol_act, \"Valor fitness\", fitness(sol_act, peso_max, pesos_valores))"
      ],
      "metadata": {
        "id": "u20HJHhkAeld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sol_ini = transformar_soluciones_no_factibles(trainY_Gr_FP[1], peso_max, pesos_valores)\n",
        "y_pred = final_model.predict(np.array([concatenar_solucion_instancia(trainX_Gr_FP_scaled[0][:int(numItems*2+1)],sol_ini )]),verbose=0)\n",
        "umbral = 0.5\n",
        "y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "sol_fact = transformar_soluciones_no_factibles(y_pred_binary[0], peso_max, pesos_valores)\n",
        "sol_act = sol_fact.tolist()\n",
        "print(\"Solución Inicial:\",sol_ini, \"Valor fitness\", fitness(sol_ini, peso_max, pesos_valores))\n",
        "print(\"Solución generada\",sol_act, \"Valor fitness\", fitness(sol_act, peso_max, pesos_valores))"
      ],
      "metadata": {
        "id": "N6IQw-kjAeld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sol_ini = transformar_soluciones_no_factibles(trainY_Gr_FP[3], peso_max, pesos_valores)\n",
        "y_pred = final_model.predict(np.array([concatenar_solucion_instancia(trainX_Gr_FP_scaled[0][:int(numItems*2+1)],sol_ini )]),verbose=0)\n",
        "umbral = 0.5\n",
        "y_pred_binary = (y_pred >= umbral).astype(int)\n",
        "sol_fact = transformar_soluciones_no_factibles(y_pred_binary[0], peso_max, pesos_valores)\n",
        "sol_act = sol_fact.tolist()\n",
        "print(\"Solución Inicial:\",sol_ini, \"Valor fitness\", fitness(sol_ini, peso_max, pesos_valores))\n",
        "print(\"Solución generada\",sol_act, \"Valor fitness\", fitness(sol_act, peso_max, pesos_valores))"
      ],
      "metadata": {
        "id": "figTH8NFBlHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}